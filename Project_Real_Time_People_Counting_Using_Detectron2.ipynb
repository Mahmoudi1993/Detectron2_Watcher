{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahmoudi1993/Detectron2_Watcher/blob/main/Project_Real_Time_People_Counting_Using_Detectron2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6fP18gzrZbu"
      },
      "source": [
        "# Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh9j8jXxrTH2"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U torch torchvision cython\n",
        "!pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip install supervision==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A27b3pS6rewm"
      },
      "outputs": [],
      "source": [
        "# Install Detectron2\n",
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the file from Gihub"
      ],
      "metadata": {
        "id": "tSaDAvSLBSnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mahmoudi1993/Detectron2_Watcher.git"
      ],
      "metadata": {
        "id": "lEYkscqEBcl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Detectron"
      ],
      "metadata": {
        "id": "Y9vC2xVZBihh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Detectron2_Watcher/"
      ],
      "metadata": {
        "id": "TIH3VR1CB4Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loader the video"
      ],
      "metadata": {
        "id": "Mm3SaHFREbRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIdeUNRXSQPz"
      },
      "outputs": [],
      "source": [
        "#ESCALATOR_VIDEO_PATH = \"  \"\n",
        "INPUT_VIDEO_PATH = \"/datasets/Cars.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the Detectron2."
      ],
      "metadata": {
        "id": "5hwW8CyYEjsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZbXtAsMpA1-"
      },
      "outputs": [],
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import torch, torchvision\n",
        "from IPython import display\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference with a pretrained model\n",
        "In this first \"coding\" part are two important utils from Detector2. On the one hand we are using cfg or better configs which represents the complete configuration of a object detection model. These configurations are stored within a YAML-file and can be easily received from the modelzoo.\n",
        "\n",
        "After the configuration is complete we'll use the DefaultPredictor class to make predictions."
      ],
      "metadata": {
        "id": "NELHEeXjFSS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mx_qrEtpUwd",
        "outputId": "a2adbff3-ed45-456b-a776-ccb5450cdb3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/11 13:12:35 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:00, 208MB/s]                           \n"
          ]
        }
      ],
      "source": [
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can either use the https://dl.fbaipublicfiles.... url, or use the following shorthand\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what number stands for which class?**\n",
        "\n",
        "Every dataset is associated with metadata. It is a key-value mapping that contains information about\n",
        "the dataset.( mapping: MetadataCatalog.get(cfg.DATASETS.TRAIN[0]) )\n",
        "It can be used to further interpret the dataset. This information can later be used for data augmentation, evaluation, visualization, logging, ..."
      ],
      "metadata": {
        "id": "cDdPj9VZIQ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# which objects it can recognize:\n",
        "# classes_id\n",
        "import pandas as pd\n",
        "modelclasses = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes\n",
        "df = pd.DataFrame(modelclasses,columns=['Model classes'])\n",
        "df"
      ],
      "metadata": {
        "id": "t7bpO9bRJpvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pstw26cUOuXL"
      },
      "outputs": [],
      "source": [
        "# Without_classes_id_person\n",
        "\n",
        "# extract video frame\n",
        "generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n",
        "iterator = iter(generator)\n",
        "frame = next(iterator)\n",
        "\n",
        "\n",
        "# detect\n",
        "outputs = predictor(frame)\n",
        "detections = sv.Detections(\n",
        "    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n",
        "    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n",
        "    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",
        ")\n",
        "\n",
        "# annotate\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "frame = box_annotator.annotate(scene=frame, detections=detections)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaT6HQ3BOvTa"
      },
      "outputs": [],
      "source": [
        "# With_classes_id_person\n",
        "\n",
        "# extract video frame\n",
        "generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n",
        "iterator = iter(generator)\n",
        "frame = next(iterator)\n",
        "\n",
        "# detect\n",
        "outputs = predictor(frame)\n",
        "detections = sv.Detections(\n",
        "    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n",
        "    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n",
        "    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",
        ")\n",
        "detections = detections[detections.class_id == 0]\n",
        "\n",
        "# annotate\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e-Y1d32O2wq",
        "outputId": "a6925138-f7ad-497b-a630-0473cb74e54e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VideoInfo(width=1920, height=1080, fps=30, total_frames=5103)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# information input_video\n",
        "sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Coordinates for a Polygon Zone**\n",
        "\n",
        "we’re going to show how to count objects in a zone using supervision and PolygonZone, an accompanying tool for calculating the coordinates for zones in an image or video.\n",
        "\n",
        "Before we can start counting objects in a zone, we need to first define the zone in which we want to count objects.\n",
        "\n",
        "To calculate coordinates inside a zone, we can use PolygonZone, an interactive web application that lets you draw polygons on an image and export their coordinates for use with supervision."
      ],
      "metadata": {
        "id": "2jRosmJGQj9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PolgyonZone needs a frame from the video with which you will be working.\n",
        "# We can extract a frame from our video using the following code:\n",
        "\n",
        "import supervision as sv\n",
        "import cv2\n",
        "\n",
        "generator = sv.get_video_frames_generator(\"./mall.mp4\")\n",
        "iterator = iter(generator)\n",
        "frame = next(iterator)\n",
        "\n",
        "cv2.imwrite(\"frame.jpg\", frame)"
      ],
      "metadata": {
        "id": "oNVTNIdsTwu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use this image to calculate the coordinates of the zone we want to draw on our image. First, open up PolygonZone (\n",
        "https://roboflow.github.io/polygonzone/ )\n",
        "and upload the frame.\n",
        "\n",
        "Then, click on points in the image where you want to draw lines. When you have drawn the full zone, click “enter” to connect the dots between the start and finish point.\n",
        "\n",
        "Once you have added your points, a NumPy array will be available on the page. This array contains the coordinates for the points in our zone."
      ],
      "metadata": {
        "id": "SUdLoOfAV1Ee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3N9r4FXO6xq"
      },
      "outputs": [],
      "source": [
        "# With the zone coordinates ready, we can now start counting objects in the zone.\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "# initiate polygon zone SUBWAY\n",
        "polygon = np.array([\n",
        "           [10, 1054],\n",
        "           [986, 774],\n",
        "           [1870, 878],\n",
        "           [1614, 1062],\n",
        "           [30, 1050]\n",
        "])\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)\n",
        "zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "# initiate annotators\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n",
        "\n",
        "# extract video frame\n",
        "generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n",
        "iterator = iter(generator)\n",
        "frame = next(iterator)\n",
        "\n",
        "# detect\n",
        "outputs = predictor(frame)\n",
        "detections = sv.Detections(\n",
        "    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n",
        "    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n",
        "    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",
        ")\n",
        "detections = detections[detections.class_id == 0]\n",
        "zone.trigger(detections=detections)\n",
        "\n",
        "# annotate\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n",
        "frame = zone_annotator.annotate(scene=frame)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUvS0z9UPAzi"
      },
      "outputs": [],
      "source": [
        "# People Counting on vidoe\n",
        "# Inference time will depend on how long your video is and the hardware on which you are running.\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "# initiate polygon zone\n",
        "polygon = np.array([\n",
        "           [10, 1054],\n",
        "           [986, 774],\n",
        "           [1870, 878],\n",
        "           [1614, 1062],\n",
        "           [30, 1050]\n",
        "])\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)\n",
        "zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n",
        "\n",
        "# initiate annotators\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n",
        "\n",
        "def process_frame(frame: np.ndarray, i: int) -> np.ndarray:\n",
        "    print('frame', i)\n",
        "    # detect\n",
        "    outputs = predictor(frame)\n",
        "    detections = sv.Detections(\n",
        "        xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n",
        "        confidence=outputs[\"instances\"].scores.cpu().numpy(),\n",
        "        class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",
        "    )\n",
        "    detections = detections[detections.class_id == 0]\n",
        "    zone.trigger(detections=detections)\n",
        "\n",
        "    # annotate\n",
        "    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "    frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n",
        "    frame = zone_annotator.annotate(scene=frame)\n",
        "\n",
        "    return frame\n",
        "\n",
        "sv.process_video(source_path=SUBWAY_VIDEO_PATH, target_path=\"/content/gdrive/MyDrive/ESCALATOR-0-result.mp4\", callback=process_frame)\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2/3N4ag5aAPTnHQSqAHpS",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}